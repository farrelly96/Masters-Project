{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b7c30de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\farre\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from packages import v2models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887c40e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DAM Price data\n",
    "\n",
    "def read_price_data(filename='Datasets/DAM_Prices_2021-Sep23.csv', columns=['DeliveryPeriod', 'EURPrices'], date_format='%m/%d/%Y %H:%M'):\n",
    "    \"\"\"\n",
    "    Reads in day-ahead market prices from a single CSV file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        filename : str\n",
    "            Filename containing day-ahead market price data.\n",
    "        columns : list of str\n",
    "            List of columns to read from the file.\n",
    "        date_format : str, default='%m/%d/%Y %H:%M'\n",
    "            Date format to be parsed from string to datetime for the date columns in the dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        price_data : pandas.DataFrame\n",
    "            Pre-processed DAM price data.\n",
    "    \"\"\"\n",
    "    # Print the columns available in the file for debugging\n",
    "    temp_df = pd.read_csv(filename, nrows=5)\n",
    "    print(f\"Available columns in the CSV file: {temp_df.columns.tolist()}\")\n",
    "\n",
    "    # Ensure the columns match those in the CSV\n",
    "    if not all(col in temp_df.columns for col in columns):\n",
    "        raise ValueError(f\"Specified columns {columns} do not match the columns in the CSV file.\")\n",
    "\n",
    "    # Create date parser\n",
    "    date_parse = lambda date: dt.datetime.strptime(date, date_format) + dt.timedelta(hours=1)\n",
    "    \n",
    "    # Read in dataset\n",
    "    price_data = pd.read_csv(filename, usecols=columns, parse_dates=['DeliveryPeriod'], date_parser=date_parse)\n",
    "    price_data.set_index('DeliveryPeriod', inplace=True)\n",
    "    \n",
    "    # Do DST adjustment - 23-hour days have their missing hour replaced with the average of the two days before and after the missing hour.\n",
    "    #                     25-hour days have the two same hours replaced by their average.\n",
    "    price_data = price_dst_adjustment(price_data)\n",
    "\n",
    "    return price_data\n",
    "\n",
    "def find_dst_index(time_step_id_dataframe, number_of_hours):\n",
    "    \"\"\"\n",
    "    Given a set of datetime.hour values for a given date (freq='H'), return the index/hour that is either missing (if\n",
    "    number_of_hours==23) or appears twice (if number_of_hours==25).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        time_step_id_dataframe : pandas.core.indexes.numeric.Int64Index\n",
    "        number_of_hours : int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        time_step_id : int \n",
    "            The missing or duplicated hour for the given DST day.\n",
    "    \"\"\"\n",
    "    if number_of_hours == 23:\n",
    "        for i, time_step_id in enumerate(time_step_id_dataframe):\n",
    "            if i < time_step_id:\n",
    "                return i + 1\n",
    "            elif i == number_of_hours - 1:\n",
    "                return 23\n",
    "                \n",
    "    elif number_of_hours == 25:\n",
    "        for j, time_step_id in enumerate(time_step_id_dataframe):\n",
    "            if j + 1 > time_step_id:\n",
    "                return time_step_id\n",
    "\n",
    "def price_dst_adjustment(df):\n",
    "    \"\"\" \n",
    "    Given a dataframe (of electricity prices), make DST adjustments so that days with 23 or 25 hours\n",
    "    are imputed/reduced to 24 hours using simple averaging.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        df : pandas.DataFrame\n",
    "            Unadjusted electricity prices dataframe.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        df : pandas.DataFrame\n",
    "            Adjusted electricity prices dataframe.\n",
    "    \"\"\"\n",
    "    df_count = df.groupby([df.index.date]).count()\n",
    "    dst_dates = df_count.loc[(df_count['EURPrices']) != 24, :]\n",
    "\n",
    "    if dst_dates.shape[0] == 0:\n",
    "        return df\n",
    "\n",
    "    for i in range(dst_dates.shape[0]):\n",
    "        dst_date = dst_dates.index[i]\n",
    "        number_of_hours = dst_dates.iloc[i, 0]\n",
    "        df_dst_data = df.loc[df.index.date == dst_date]\n",
    "        dst_index = find_dst_index(df_dst_data.index.hour, number_of_hours)\n",
    "        \n",
    "        if number_of_hours == 23:\n",
    "            previous_price = df_dst_data.loc[df_dst_data.index.hour == dst_index - 1]\n",
    "            next_day_price = df.loc[df.index.date == dst_date + dt.timedelta(days=1)]\n",
    "            next_price = next_day_price.loc[next_day_price.index.hour == 0]\n",
    "            adjacent_prices = pd.concat([previous_price, next_price])\n",
    "            average_values = adjacent_prices.mean(axis=0).values[0]\n",
    "            new_price = pd.DataFrame(dict(EURPrices=average_values), index=[dt.datetime.combine(dst_date, dt.datetime.min.time()) + dt.timedelta(hours=dst_index)])\n",
    "            df = pd.concat([df, new_price])\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "            df.index.name = 'DeliveryPeriod'\n",
    "            \n",
    "        elif number_of_hours == 25:\n",
    "            duplicate_prices = df_dst_data.loc[df_dst_data.index.hour == dst_index]\n",
    "            average_values = duplicate_prices.mean(axis=0).values[0]\n",
    "            df.drop(duplicate_prices.index, inplace=True)\n",
    "            new_price = pd.DataFrame(dict(EURPrices=average_values), index=[dt.datetime.combine(dst_date, dt.datetime.min.time()) + dt.timedelta(hours=dst_index)])\n",
    "            df = pd.concat([df, new_price])\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    df.sort_index(axis=0, inplace=True)\n",
    "    df = df.loc[~df.index.duplicated()]\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "796ba3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDA price data\n",
    "\n",
    "def read_intraday_price_data(filenames=['Datasets/IDA1_Prices_2021-Sep23.csv', 'Datasets/IDA2_Prices_2021-Sep23.csv', 'Datasets/IDA3_Prices_2021-Sep23.csv'], \n",
    "                             columns=['timestamp', 'price_eur'], \n",
    "                             date_format='%m/%d/%Y %H:%M'):\n",
    "    \"\"\"\n",
    "    Reads in the intraday auction prices from Datasets directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        filenames : list of str\n",
    "            List of filenames containing intraday auction price data.\n",
    "        columns : list of str\n",
    "            List of columns to read from the files.\n",
    "        date_format : str, default='%m/%d/%Y %H:%M'\n",
    "            Date format to be parsed from string to datetime for the date columns in the dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        ida_data : pandas.DataFrame\n",
    "            Pre-processed intraday auction price data.\n",
    "    \"\"\"\n",
    "    # Create date parser\n",
    "    date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "\n",
    "    # Read in datasets and rename the price column to differentiate between IDA1, IDA2, and IDA3\n",
    "    ida_data_frames = []\n",
    "    for i, filename in enumerate(filenames):\n",
    "        ida_data = pd.read_csv(filename, usecols=columns, parse_dates=True, index_col='timestamp', date_parser=date_parse)\n",
    "        ida_data.columns = [f'IDA{i+1}_Price']\n",
    "        ida_data_frames.append(ida_data)\n",
    "\n",
    "    # Combine datasets\n",
    "    ida_data = reduce(lambda left, right: left.join(right, how='outer'), ida_data_frames)\n",
    "\n",
    "    # Remove duplicates\n",
    "    ida_data = ida_data.loc[~ida_data.index.duplicated()]\n",
    "\n",
    "    # Remove last day of data if last delivery hour is not 23:00\n",
    "    if ida_data.index[-1].hour != 21:\n",
    "        last_date = dt.datetime.combine(ida_data.index[-1].date(), dt.datetime.min.time()) - dt.timedelta(hours=1)\n",
    "        ida_data = ida_data.loc[:last_date]\n",
    "\n",
    "    # Get list of dates that do not have exactly 24 data points (hours)\n",
    "    count_df = ida_data.groupby(ida_data.index.date).count()\n",
    "    missing_values = count_df.loc[(count_df != 24).any(axis=1)]\n",
    "\n",
    "    # Clean data - Use the backfill and forward fill method to deal with missing values\n",
    "    \n",
    "    ida_data = ida_data.fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # Relabel the index\n",
    "    ida_data.index.name = 'DeliveryPeriod'\n",
    "\n",
    "    return(ida_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cd8af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fuel Mix data\n",
    "\n",
    "def read_fuel_mix_data(filename='Datasets/Fuel_Mix.csv', \n",
    "                             columns=['Period', 'Battery','CCGT','CHP','DSR','Hydro','Interconnectors','OCGT','Oil','Peat','Pumped_Storage','Solar_Actual','Waste','Wind_Actual'], \n",
    "                             date_format='%d/%m/%Y %H:%M'):\n",
    "    \"\"\"\n",
    "    Reads in fuel mix from Datasets directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        filename : list of str\n",
    "            List of filenames containing fuel mix data.\n",
    "        columns : list of str\n",
    "            List of columns to read from the file.\n",
    "        date_format : str, default='%d/%m/%Y %H:%M'\n",
    "            Date format to be parsed from string to datetime for the date columns in the dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        fuel_mix_data : pandas.DataFrame\n",
    "            Pre-processed fuel mix data provided by EnAppSys.\n",
    "    \"\"\"\n",
    "    # Create date parser\n",
    "    date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "\n",
    "    # Read in dataset\n",
    "    fuel_mix_data = pd.read_csv(filename, usecols=columns, parse_dates=True, index_col='Period', date_parser=date_parse)\n",
    "    \n",
    "    # Remove last day of data if last delivery hour is not 23:00\n",
    "    if fuel_mix_data.index[-1].hour != 21:\n",
    "        last_date = dt.datetime.combine(fuel_mix_data.index[-1].date(), dt.datetime.min.time()) - dt.timedelta(hours=1)\n",
    "        fuel_mix_data = fuel_mix_data.loc[:last_date]\n",
    "\n",
    "    # Get list of dates that do not have exactly 24 data points (hours)\n",
    "    count_df = fuel_mix_data.groupby(fuel_mix_data.index.date).count()\n",
    "    missing_values = count_df.loc[(count_df != 24).any(axis=1)]\n",
    "    \n",
    "    # Relabel the index\n",
    "    fuel_mix_data.index.name = 'DeliveryPeriod'\n",
    "\n",
    "    return(fuel_mix_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436431c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gas_price_data(filename='Datasets/TTF_gas_prices.csv', \n",
    "                             columns=['Period', 'GasPrice'], \n",
    "                             date_format='%m/%d/%Y %H:%M'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Reads in gas price from Datasets directory. TTF gas futures used as proxy for gas price.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        filename : list of str\n",
    "            List of filenames containing gas price data.\n",
    "        columns : list of str\n",
    "            List of columns to read from the files.\n",
    "        date_format : str, default='%m/%d/%Y %H:%M'\n",
    "            Date format to be parsed from string to datetime for the date columns in the dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        gas_price_data : pandas.DataFrame\n",
    "            Pre-processed gas price data.\n",
    "    \"\"\"\n",
    "    # Create date parser\n",
    "    date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "    \n",
    "    #Read in Dataset\n",
    "    gas_price_data = pd.read_csv(filename, usecols=columns, parse_dates=True, index_col='Period', date_parser=date_parse)\n",
    "    \n",
    "    #Set Index name\n",
    "    gas_price_data.index.name = 'DeliveryPeriod'\n",
    "\n",
    "    return(gas_price_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ac3dd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demand/Wind/Solar forecast data\n",
    "\n",
    "def read_forecast_data(forecast_type, filename=None, forecast_column=['Period', 'AggregatedForecast'], date_format='%m/%d/%Y %H:%M'):\n",
    "    \"\"\"\n",
    "    Reads in forecast demand, solar or wind load from Datasets directory.\n",
    "    Parameters\n",
    "    ----------\n",
    "        forecast_type : string in ['Wind', 'Forecast', 'Solar']\n",
    "        filename : list of str\n",
    "            List of filenames containing the forecast data.\n",
    "        forecast_column : list of str\n",
    "            List of columns to read from the files.\n",
    "        date_format : str, default='%Y-%m-%d %H:%M:%S'\n",
    "            Date format to be parsed from string to datetime for the date columns in the dataset.\n",
    "    Returns\n",
    "    -------\n",
    "        forecast_data : pandas.DataFrame\n",
    "            Pre-processed forecast data provided by EnAppSys.\n",
    "    \"\"\"\n",
    "    # Set the filenames to be read in when forecast_type is specified.\n",
    "    if filename is None:\n",
    "        if forecast_type == 'Wind':\n",
    "            filename = ['Datasets/Onshore_Wind_Forecast1Sep22-Oct23.csv', 'Datasets/Onshore_Wind_Forecast2Sep22-Oct23.csv']\n",
    "        elif forecast_type == 'Demand':\n",
    "            filename = ['Datasets/Demand_Forecast1Sep22-Oct23.csv', 'Datasets/Demand_Forecast2Sep22-Oct23.csv']\n",
    "        elif forecast_type == 'Solar':\n",
    "            filename = ['Datasets/Solar_Forecast1Sep22-Oct23.csv', 'Datasets/Solar_Forecast2Sep22-Oct23.csv']\n",
    "\n",
    "    # Create date parser\n",
    "    date_parse = lambda date: dt.datetime.strptime(date, date_format) + dt.timedelta(hours=1)\n",
    "\n",
    "    # Read in dataset/s\n",
    "    forecast_data1 = pd.read_csv(filename[0], usecols=forecast_column, parse_dates=True, index_col='Period', date_parser=date_parse)\n",
    "    forecast_data2 = pd.read_csv(filename[1], usecols=forecast_column, parse_dates=True, index_col='Period', date_parser=date_parse)\n",
    "\n",
    "    # Combine dataset/s\n",
    "    forecast_data = pd.concat([forecast_data1, forecast_data2])\n",
    "\n",
    "    # Remove duplicates\n",
    "    forecast_data = forecast_data.loc[[not val for val in forecast_data.index.duplicated()]]\n",
    "\n",
    "    # Rename index and column\n",
    "    forecast_data.index.name = 'DeliveryPeriod'\n",
    "    forecast_data.columns = [forecast_type]\n",
    "\n",
    "    return(forecast_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "428ca2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk-forward evaluation of a forecasting model\n",
    "\n",
    "def walk_forward_evaluation(model, price_data, ida_data=None, planned_data=None, fuel_mix_data=None, gas_price_data=None, starting_window_size=None, moving_window=False, start=None, end=None, logs=True):\n",
    "    \"\"\"\n",
    "    Evalutes a forecasting model using the given price_data and (optional) ida market data,\n",
    "    wind/solar/demand forecast data, fuel mix data and gas price data. The general procedure is as follows:\n",
    "        \n",
    "        * Create an initial training data window. It is important to that only the data that would be available the day before\n",
    "          the first forecast date given by the start parameter is used.\n",
    "        * For each forecast_date in dates_between(start, end):\n",
    "            - Ingest data. The data required by the model is reformatted (as needed) into suitable train/test input\n",
    "              and stored in the model object for later training/forecasting.\n",
    "            - Train. The model is trained to the available data prior to forecast_date.\n",
    "            - Forecast. Forecast is generated for forecast_date.\n",
    "            - Store forecasts in a dataframe.\n",
    "        * Calculate RMSE and MAE for the whole period.\n",
    "        * Return model object, model forecasts, RMSE and MAE.\n",
    "\n",
    "    The above procedure applies for the following model classes: naive(), random_forest(), ARX(), SARIMAX(), ffnn(), rnn().\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        model : class\n",
    "            Class object defining the forecasting model, with class methods\n",
    "            self.ingest_data(), self.train() and self.forecast().\n",
    "        price_data : pandas.DataFrame\n",
    "            Electricity price data.\n",
    "        ida_data : pandas.DataFrame\n",
    "            Intraday Auction market price data.\n",
    "        planned_data : pandas.DataFrame\n",
    "            Wind, Solar and demand forecast data.\n",
    "        fuel_mix_data : pandas.DataFrame\n",
    "            DAM supply and demand curve data.\n",
    "        gas_price_data: pandas.DataFrame\n",
    "            TTF futures gas price data used as proxy for gas data.\n",
    "        starting_window_size : int\n",
    "            number of days of data to start the training set on, i.e. training set size\n",
    "            for first forecast iteration on start date.\n",
    "        moving_window : bool\n",
    "            To specify whether the training window is a moving window (True) or an expanding window (False).\n",
    "        start : datetime.datetime\n",
    "            Date on which to start the walk-forward validation.\n",
    "        end : datetime.datetime\n",
    "            Date to end the walk-forward validation on (inclusive).\n",
    "        logs : bool, default=True\n",
    "            Specifies whether to print overall execution time of the walk-forward validation\n",
    "            for the entire start-to-end period. True prints out the logs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        model : class\n",
    "            The model object after it has been modified by the walk_forward_evaluation() function.\n",
    "        forecasts_df : pandas.DataFrame\n",
    "            DataFrame of original prices and corresponding model forecasts\n",
    "        rmse : int\n",
    "            Root-mean-square error (RMSE) for the entire period represented by forecasts_df\n",
    "        mae : int\n",
    "            Mean absolute error (MAE) for the entire period represented by forecasts_df\n",
    "\"\"\"\n",
    "    if logs:\n",
    "        start_time = time.time()\n",
    "    \n",
    "    # Validation to ensure start date <= end date.\n",
    "    if start is not None and end is not None:\n",
    "        if start > end:\n",
    "            raise Exception(f\"Cannot have start date after end date.\")\n",
    "    \n",
    "    # Validation to ensure the number of days of training data is at least starting_window_size.\n",
    "    if starting_window_size is not None:\n",
    "        if (start-price_data.index[0]).days < starting_window_size:\n",
    "            raise Exception(f\"Not enough data for training: starting_window_size={starting_window_size}, train_size={start-data.index[0]}\")\n",
    "    \n",
    "    # Fetch datetime index for initial training data window\n",
    "    if starting_window_size is None:\n",
    "        train_dates = list(pd.date_range(start=price_data.index[0], end=start-dt.timedelta(hours=1), freq='H'))\n",
    "    else:\n",
    "        train_dates = list(pd.date_range(end=start-dt.timedelta(hours=1), periods=24*starting_window_size, freq='H'))\n",
    "\n",
    "    # Get initial price data\n",
    "    train_price_data = price_data.loc[train_dates,:]\n",
    "    \n",
    "    # Get initial IDA market data\n",
    "    train_ida_data = None if ida_data is None else ida_data.loc[train_dates,:]\n",
    "    \n",
    "    # Fetch datetime index for initial planned (wind, solar & forecast) data\n",
    "    if planned_data is not None:\n",
    "        if starting_window_size is None:\n",
    "            train_planned_dates = pd.date_range(start=planned_data.index[0], end=start+dt.timedelta(hours=23), freq='H')\n",
    "        else:\n",
    "            train_planned_dates = pd.date_range(end=start+dt.timedelta(hours=23), periods=24*(starting_window_size+1), freq='H')\n",
    "    \n",
    "    # Get initial planned (wind, solar & demand) data\n",
    "    train_planned_data = None if planned_data is None else planned_data.loc[train_dates,:]\n",
    "    \n",
    "    # Fetch datetime index for initial planned (wind, solar & forecast) data\n",
    "    if fuel_mix_data is not None:\n",
    "        if starting_window_size is None:\n",
    "            train_fuel_mix_dates = pd.date_range(start=planned_data.index[0], end=start+dt.timedelta(hours=23), freq='H')\n",
    "        else:\n",
    "            train_fuel_mix_dates = pd.date_range(end=start+dt.timedelta(hours=23), periods=24*(starting_window_size+1), freq='H')\n",
    "    \n",
    "    # Get initial fuel mix data\n",
    "    train_fuel_mix_data = None if fuel_mix_data is None else fuel_mix_data.loc[train_dates,:]\n",
    "    \n",
    "#     # Fetch datetime index for initial gas price data\n",
    "    if gas_price_data is not None:\n",
    "        if starting_window_size is None:\n",
    "            train_gas_price_dates = pd.date_range(start=planned_data.index[0], end=start+dt.timedelta(hours=23), freq='H')\n",
    "        else:\n",
    "            train__dates_gas_price= pd.date_range(end=start+dt.timedelta(hours=23), periods=24*(starting_window_size+1), freq='H')\n",
    "    \n",
    "    # Get initial gas price data\n",
    "    train_gas_price_data = gas_price_data.loc[train_dates,:]\n",
    "        \n",
    "    # Create dataframe to store errors\n",
    "    forecast_index = pd.date_range(start=start, end=end+dt.timedelta(hours=23), freq='H')\n",
    "    forecasts_df = pd.DataFrame(columns=['Forecast'], index=forecast_index)\n",
    "    forecasts_df.insert(0, 'Original', price_data['EURPrices'].loc[forecast_index])\n",
    "    \n",
    "    # Debugging print for initial setup\n",
    "    print(\"Initial setup complete.\")\n",
    "    print(f\"Train Price Data Index: {train_price_data.index}\")\n",
    "    if ida_data is not None:\n",
    "        print(f\"Train IDA Data Index: {train_ida_data.index}\")\n",
    "    if planned_data is not None:\n",
    "        print(f\"Train Planned Data Index: {train_planned_data.index}\")\n",
    "    if fuel_mix_data is not None:\n",
    "        print(f\"Train Fuel Mix Data Index: {train_fuel_mix_data.index}\")\n",
    "    if gas_price_data is not None:\n",
    "        print(f\"Train Gas Price Data Index: {train_gas_price_data.index}\")\n",
    "    \n",
    "    # Loop through data to train and forecast iteratively over the expanding (or moving) window\n",
    "    for i in range((end-start).days+1):\n",
    "        current_time = start + dt.timedelta(days=i)\n",
    "        next_day = current_time + dt.timedelta(days=1)\n",
    "        \n",
    "        # Ingest data\n",
    "        if fuel_mix_data is not None:\n",
    "            model.ingest_data(train_price_data, train_ida_data, train_planned_data, train_fuel_mix_data, train_gas_price_data)\n",
    "        else:\n",
    "            model.ingest_data(train_price_data, train_ida_data, train_planned_data, None)\n",
    "        \n",
    "        # Train model\n",
    "        model.train()\n",
    "        \n",
    "        # Generate day-ahead forecast and store in forecasts_df dataframe\n",
    "        forecast = model.forecast()\n",
    "        \n",
    "        # Adjust forecast index to match the expected date range\n",
    "        forecast_index = pd.date_range(start=current_time, end=next_day-dt.timedelta(hours=1), freq='H')\n",
    "#         forecast = pd.DataFrame(forecast, index=forecast_index)\n",
    "        \n",
    "        # Debugging prints before updating forecasts_df\n",
    "        print(f\"Current iteration: {i}\")\n",
    "        print(f\"Forecast index: {forecast.index if hasattr(forecast, 'index') else 'N/A'}\")\n",
    "        print(f\"Forecast values: {forecast.values}\")\n",
    "        \n",
    "        try:\n",
    "            forecasts_df.loc[forecast.index, 'Forecast'] = forecast.values\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError encountered: {e}\")\n",
    "            print(f\"Current Time: {start + dt.timedelta(days=i)}\")\n",
    "            print(f\"Forecasts DataFrame Index: {forecasts_df.index}\")\n",
    "            print(f\"Forecast index: {forecast.index}\")\n",
    "            print(f\"Forecast values: {forecast.values}\")\n",
    "            raise\n",
    "        forecast_value = forecast.iloc[0, 0]\n",
    "        \n",
    "        # Drop last day of data if moving_window=True\n",
    "        if moving_window:\n",
    "            train_price_data.drop(train_price_data.index[:24], inplace=True)\n",
    "            if ida_data is not None:\n",
    "                train_ida_data.drop(train_ida_data.index[:24], inplace=True)\n",
    "            if planned_data is not None:\n",
    "                train_planned_data.drop(train_planned_data.index[:24], inplace=True)\n",
    "            if fuel_mix_data is not None:\n",
    "                train_fuel_mix_data.drop(train_planned_data.index[:24], inplace=True)\n",
    "            if gas_price_data is not None:\n",
    "                train_gas_price_data.drop(train_planned_data.index[:24], inplace=True)\n",
    "        \n",
    "        # Get datetime index for new date of data to be added to training data\n",
    "        next_date = list(pd.date_range(start=train_price_data.index[-1]+dt.timedelta(hours=1), periods=24, freq='H'))\n",
    "        \n",
    "        # Fetch new DAM prices data and add to training data\n",
    "        new_price_data = price_data.loc[next_date,:]\n",
    "        train_price_data = pd.concat([train_price_data, new_price_data])\n",
    "        \n",
    "        print(f\"Finished forecast for {forecast.index.date[0]}. Forecast value: {forecast_value}.\")\n",
    "        \n",
    "        # Fetch new IDA prices data and add to training data\n",
    "        if ida_data is not None:\n",
    "            new_ida_data = ida_data.loc[next_date,:]\n",
    "            train_ida_data = pd.concat([train_ida_data, new_ida_data])\n",
    "            \n",
    "        # Fetch new forecast (wind & demand) data and add to training data\n",
    "        if planned_data is not None:\n",
    "            #next_planned_date = list(pd.date_range(start=train_planned_data.index[-1]+dt.timedelta(hours=1), periods=24, freq='H'))\n",
    "            new_planned_data = planned_data.loc[next_date,:]\n",
    "            train_planned_data = pd.concat([train_planned_data, new_planned_data])\n",
    "            \n",
    "        # Fetch new fuel mix data and add to training data\n",
    "        if fuel_mix_data is not None:\n",
    "            #next_fuel_mix_date = list(pd.date_range(start=train_fuel_mix_data.index[-1]+dt.timedelta(hours=1), periods=24, freq='H'))\n",
    "            new_fuel_mix_data = fuel_mix_data.loc[next_date,:]\n",
    "            train_fuel_mix_data = pd.concat([train_fuel_mix_data, new_fuel_mix_data])\n",
    "        \n",
    "        # Fetch new gas price data and add to training data\n",
    "        if gas_price_data is not None:\n",
    "            #next_gas_price_date = list(pd.date_range(start=train_gas_price_data.index[-1]+dt.timedelta(hours=1), periods=24, freq='H'))\n",
    "            new_gas_price_data = gas_price_data.loc[next_date,:]\n",
    "            train_gas_price_data = pd.concat([train_gas_price_data, new_gas_price_data])\n",
    "            \n",
    "    # Calculate RMSE and MAE for the entire period of the walk-forward evaluation\n",
    "    rmse = metrics.mean_squared_error(forecasts_df['Original'], forecasts_df['Forecast'], squared=False)\n",
    "    mae = metrics.mean_absolute_error(forecasts_df['Original'], forecasts_df['Forecast'])\n",
    "    \n",
    "    if logs:\n",
    "        print(f\"Execution time: {time.time()-start_time} seconds\")\n",
    "        \n",
    "    return(model, forecasts_df, rmse, mae)\n",
    "\n",
    "\n",
    "def get_resampled_errors(res_df, index_filter):\n",
    "    \"\"\"\n",
    "    This takes a dataframe of forecasted (and original) hourly electricity price values\n",
    "    and calculates rmse/mae values across different sampling rates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        res_df : pandas.DataFrame\n",
    "            DataFrame of original and forecast prices.\n",
    "        index_filter : string in ['date', 'dayofweek_and_hour', \"'ayofweek', 'hour', 'month']\n",
    "            Sampling rate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        errors : pandas.DataFrame\n",
    "    \"\"\"\n",
    "    # Group dataframe values by index, with grouping determined by index_filter\n",
    "    if index_filter == 'date':\n",
    "        group = res_df.groupby(res_df.index.date)\n",
    "    elif index_filter == 'dayofweek_and_hour':\n",
    "        group = res_df.groupby([res_df.index.hour, res_df.index.dayofweek])\n",
    "    elif index_filter == 'dayofweek':\n",
    "        group = res_df.groupby(res_df.index.dayofweek)\n",
    "    elif index_filter == 'hour':\n",
    "        group = res_df.groupby(res_df.index.hour)\n",
    "    elif index_filter == 'month':\n",
    "        group = res_df.groupby(res_df.index.month)\n",
    "        \n",
    "    # Calculate RMSEs and MAEs for each group period\n",
    "    rmses = group.apply(lambda df: metrics.mean_squared_error(df['Original'], df['Forecast'], squared=False))\n",
    "    maes = group.apply(lambda df: metrics.mean_absolute_error(df['Original'], df['Forecast']))\n",
    "    \n",
    "    # Combine RMSEs and MAEs dataframes into one dataframe\n",
    "    errors = pd.concat([rmses, maes], axis=1)\n",
    "    errors.columns = ['RMSE', 'MAE']\n",
    "    \n",
    "    return(errors)\n",
    "\n",
    "\n",
    "def walk_forward_loop(model_func, dates_to_forecast, model_params, lag_params, hyperparameter, prices, ida_prices, planned, fuel_mix, gas_price, logs=True):\n",
    "    \"\"\"\n",
    "    A looping function to use the walk-forward validation on a set of non-adjacent (datetime) dates.\n",
    "    This function is used for Section 4.4 - Hyperparameter Tuning.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        model_func : class\n",
    "            Class object defining the forecasting model, with class methods\n",
    "            self.ingest_data(), self.train() and self.forecast().\n",
    "        dates_to_forecast : list of datetime.datetime\n",
    "            Dates to forecast on.\n",
    "        model_params : dict\n",
    "            Argument for model __init__() specifying model parameters\n",
    "        lag_params : dict\n",
    "            Argument for model __init__() specifying the data lags to be used as (exogenous) predictors\n",
    "        hyperparameter : str\n",
    "            Name of model hyperparameter to iteratively parameterise the model on.\n",
    "            Note: the corresponding hyperparameter in the model_params must be a list of possible hyperparameter\n",
    "            values to train the models on.\n",
    "        prices : pandas.DataFrame\n",
    "            Electricity price data.\n",
    "       IDA_prices : pandas.DataFrame\n",
    "            Intraday Auction market price data.\n",
    "        planned : pandas.DataFrame\n",
    "            Wind and demand forecast data.\n",
    "        fuel_mix : pandas.DataFrame\n",
    "            Fuel mix data\n",
    "        gas_price_data: pandas.DataFrame\n",
    "            TTF futures gas price data used as proxy for gas data.\n",
    "        logs : bool, default=True\n",
    "            Specifies whether to print overall execution time of the walk-forward validation\n",
    "            for the entire start-to-end period. True prints out the logs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        errors : pandas.DataFrame\n",
    "            Dataframe of overall RMSE and MAE of each hyperparameter value.\n",
    "    \"\"\"\n",
    "    # Initialise dataframe to store errors (indexed by hyperparameter values given by model_params[\"init_params\"][hyperparameter]).\n",
    "    errors_index = model_params['init_params'][hyperparameter] if model_func == v2models.ffnn else model_params[hyperparameter]\n",
    "    errors = pd.DataFrame(columns=['RMSE', 'MAE'], index=errors_index)\n",
    "    errors.index.name = hyperparameter\n",
    "    \n",
    "    # Ensure that dates_to_forecast values are of type datetime.datetime\n",
    "    dates_to_forecast = [dt.datetime.combine(date, dt.datetime.min.time()) if type(date) is dt.date else date for date in dates_to_forecast]\n",
    "\n",
    "    # Fix hyperparameter value for walk-forward validation\n",
    "    for param in errors.index:\n",
    "        new_model_params = model_params.copy()\n",
    "        \n",
    "        # Create appropriate model_params dict as input for model __init__().\n",
    "        if model_func == v2models.ffnn:\n",
    "            new_model_params['init_params'][hyperparameter] = param\n",
    "        else:\n",
    "            new_model_params[hyperparameter] = param\n",
    "\n",
    "        overall_res = pd.DataFrame(columns=['Original', 'Forecast'])\n",
    "        \n",
    "        # Run walk-forward validation for all dates in dates_to_forecast.\n",
    "        for date in dates_to_forecast:\n",
    "            # Initialise model\n",
    "            model = model_func(model_params=new_model_params, lag_params=lag_params)\n",
    "            \n",
    "            # Run walk-forward validation function for the given date in dates_to_forecast,\n",
    "            # and temporarily store the forecasts\n",
    "            _, res, _, _ = walk_forward_evaluation(model, prices, ida_prices, planned, fuel_mix, gas_price, start=date, end=date, logs=False)\n",
    "            overall_res = overall_res.append(res)\n",
    "            \n",
    "        # Calculate and store the RMSE and MAE for the forecasts over all the dates in dates_to_forecast\n",
    "        rmse = metrics.mean_squared_error(overall_res['Original'], overall_res['Forecast'], squared=False)\n",
    "        mae = metrics.mean_absolute_error(overall_res['Original'], overall_res['Forecast'])\n",
    "            \n",
    "        # Store the RMSE and MAE for the given hyperparameter.\n",
    "        errors.loc[param, 'RMSE'] = rmse\n",
    "        errors.loc[param, 'MAE'] = mae\n",
    "        \n",
    "        if logs: print(f\"Finished for {hyperparameter}={param}\")\n",
    "    \n",
    "    return(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24a69ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
