{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7ec45e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\farre\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import cloudpickle\n",
    "import matplotlib as mpl\n",
    "from functools import reduce\n",
    "import time\n",
    "import warnings\n",
    "import joblib as jl\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "import statsmodels.tsa as tsa\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "import sklearn.preprocessing as prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9eb6c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive (Benchmarks)\n",
    "\n",
    "class naive():\n",
    "    \"\"\"\n",
    "    Naive model implementation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        lag : int\n",
    "            Number of days to go back in order to make forecast, e.g. day_lag=7 indicates\n",
    "            a weekly persistent model, day_lag=1 indicates a daily persistent model, etc.\n",
    "        period : str in ['D', 'Y']\n",
    "    \"\"\"\n",
    "    def __init__(self, lag, period):\n",
    "        self.lag = lag\n",
    "        self.period = period\n",
    "    \n",
    "    def ingest_data(self, train_target, train_ida, train_planned, train_fuel_mix):\n",
    "        self.data = train_target.copy()\n",
    "       \n",
    "    def train(self):\n",
    "        # Date for which forecast will be made\n",
    "        self.target_date = dt.datetime.combine(self.data.index.date[-1], dt.datetime.min.time()) + dt.timedelta(days=1)\n",
    "    \n",
    "    def forecast(self):\n",
    "        # Make forecasts\n",
    "        if self.period == 'D':\n",
    "            forecast_df = self.data.loc[self.data.index.date == self.data.index.date[-(self.lag*24)]]\n",
    "        elif self.period == 'Y':\n",
    "            # Get corresponding date from lag years before    \n",
    "            try:\n",
    "                forecast_date = dt.datetime(self.target_date.year-1, self.target_date.month, self.target_date.day)\n",
    "            except:\n",
    "                forecast_date = dt.datetime(self.target_date.year-1, self.target_date.month, self.target_date.day-1)\n",
    "            \n",
    "            forecast_df = self.data.loc[self.data.index.date == forecast_date.date()]\n",
    "\n",
    "        # Reindex forecasts to the appropriate forecast date and relabel forecasts_df\n",
    "        forecast_df.index = pd.date_range(self.target_date, periods=24, freq='H')\n",
    "        forecast_df.index.name = 'DeliveryPeriod'\n",
    "\n",
    "        return(forecast_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54d74b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "class random_forest():\n",
    "    \"\"\"\n",
    "    Random forest model implementation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        model_params : dict\n",
    "            n_estimators : int\n",
    "                Number of trees\n",
    "            max_depth : int\n",
    "                Max tree depth\n",
    "            max_features : int\n",
    "                Number of features considered at each decision tree split\n",
    "            n_jobs : int\n",
    "                Number of processes (joblib argument for parallelisation)\n",
    "        lag_params : dict\n",
    "            price_lags : list of int\n",
    "                Lags of day-ahead market price data to use as predictors\n",
    "            ida_price_lags : list of int\n",
    "                Lags of balancing market price data to use as predictors\n",
    "            planned_lags : list of int\n",
    "                Lags of forecast data (wind, demand, solar) to use as predictors\n",
    "            \n",
    "    \"\"\"\n",
    "    def __init__(self, model_params, lag_params):\n",
    "        self.model = ensemble.RandomForestRegressor(**model_params, oob_score=True, random_state=1)\n",
    "        self.price_lags = lag_params.get('price_lags', [])\n",
    "        self.ida_price_lags = lag_params.get('ida_price_lags', [])\n",
    "        self.planned_lags = lag_params.get('planned_lags', [])\n",
    "        self.fuel_mix_lags = lag_params.get('fuel_mix_lags', [])\n",
    "        self.gas_price_lags = lag_params.get('gas_price_lags', [])\n",
    "        \n",
    "    def ingest_data(self, train_price_data, train_ida, train_forecast, train_fuel_mix, train_gas_price):\n",
    "        # Identify the latest start date among the datasets to align them\n",
    "        start_dates = [df.index[0] for df in [train_price_data, train_ida, train_forecast, train_fuel_mix, train_gas_price] if not df.empty]\n",
    "        latest_start_date = max(start_dates) if start_dates else None\n",
    "\n",
    "        # Split into training predictors and test predictors\n",
    "        last_day_of_data = dt.datetime.combine(train_forecast.index.date[-1], dt.datetime.min.time())\n",
    "        test_index = pd.date_range(start=last_day_of_data, end=last_day_of_data + dt.timedelta(hours=23), freq='H')\n",
    "    \n",
    "        # Initialise predictors dataframe\n",
    "        predictors = pd.DataFrame(index=train_forecast.index)\n",
    "        \n",
    "        # Build predictors from DAM prices\n",
    "        for lag in self.price_lags:\n",
    "            predictor_name = f\"{train_price_data.columns[0]}-{lag}\"\n",
    "            predictors.insert(predictors.shape[1], predictor_name, train_price_data)\n",
    "            predictors[predictor_name] = predictors[predictor_name].shift(lag)\n",
    "\n",
    "        # Build predictors from Wind, Demand, and Solar forecasts\n",
    "        for column in train_forecast:\n",
    "            for lag in self.planned_lags:\n",
    "                predictor_name = f\"{column}-{lag}\"\n",
    "                predictors.insert(predictors.shape[1], predictor_name, train_forecast[column])\n",
    "                predictors[predictor_name] = predictors[predictor_name].shift(lag)\n",
    "                \n",
    "        # Build predictors from IDA1, IDA2, and IDA3 prices\n",
    "        for column in train_ida:\n",
    "            for lag in self.ida_price_lags:\n",
    "                predictor_name = f\"{column}-{lag}\"\n",
    "                predictors.insert(predictors.shape[1], predictor_name, train_ida[column])\n",
    "                predictors[predictor_name] = predictors[predictor_name].shift(lag)\n",
    "                \n",
    "        # Build predictors from Fuel mix data set\n",
    "        for column in train_fuel_mix:\n",
    "            for lag in self.fuel_mix_lags:\n",
    "                predictor_name = f\"{column}-{lag}\"\n",
    "                predictors.insert(predictors.shape[1], predictor_name, train_fuel_mix[column])\n",
    "                predictors[predictor_name] = predictors[predictor_name].shift(lag)\n",
    "                \n",
    "        # Build predictors from Gas prices\n",
    "        for column in train_gas_price:\n",
    "            for lag in self.gas_price_lags:\n",
    "                predictor_name = f\"{column}-{lag}\"\n",
    "                predictors.insert(predictors.shape[1], predictor_name, train_gas_price[column])\n",
    "                predictors[predictor_name] = predictors[predictor_name].shift(lag)\n",
    "\n",
    "        # Split predictors into training and test predictors and store data for training and forecasting\n",
    "        train_predictors = predictors.drop(test_index)\n",
    "        test_predictors = predictors.loc[test_index, :]\n",
    "        \n",
    "        # Remove rows in training set with NAs\n",
    "        notna_train_predictors_loc = train_predictors.notna().all(axis=1)\n",
    "        train_predictors = train_predictors.loc[notna_train_predictors_loc]\n",
    "        train_price_data = train_price_data.loc[notna_train_predictors_loc]\n",
    "        \n",
    "        # Store data into object\n",
    "        self.train_predictors = train_predictors\n",
    "        self.test_predictors = test_predictors\n",
    "        self.train_price_data = train_price_data\n",
    "        \n",
    "        # Initialise dataframe for variable importances\n",
    "        if not hasattr(self, 'variable_importances'):\n",
    "            self.variable_importances = pd.DataFrame(index=train_predictors.columns)\n",
    "        \n",
    "    def train(self):\n",
    "        # Fit the forecasting model\n",
    "        self.model.fit(X=self.train_predictors, y=self.train_price_data['EURPrices'])\n",
    "        \n",
    "        # Store variable importances\n",
    "        variable_importances = self.model.feature_importances_\n",
    "        self.variable_importances.insert(self.variable_importances.shape[1], self.test_predictors.index.date[0], variable_importances)\n",
    "\n",
    "    def forecast(self):\n",
    "        forecast_df = pd.DataFrame(self.model.predict(self.test_predictors), index=self.test_predictors.index)\n",
    "        forecast_df.index.name = 'DeliveryPeriod'\n",
    "        return forecast_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90cd059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AR/ARX\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "    model_params: dict with keys: lags, trend, ic, exog\n",
    "    lag_params: dict with keys: ida_price_lags, planned_lags\n",
    "\"\"\"\n",
    "class ARX():\n",
    "    \"\"\"\n",
    "    AR/ARX model implementation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        model_params : dict\n",
    "            lags : list of int\n",
    "                AR/ARX model orders to fit.\n",
    "            trend : str in ['n', 'c', 't', 'ct']\n",
    "                Specifies ar_model.AutoReg() parameter for model trend, if any.\n",
    "            ic : str in ['aic', 'bic']\n",
    "                Information criterion to use in determining the 'best' model order specification.\n",
    "            exog : bool\n",
    "                Whether or not to include exogenous data (i.e. implement ARX (True) or AR (False))\n",
    "        lag_params : dict\n",
    "            ida_price_lags : list of int\n",
    "                Lags of intraday market price data to use as predictors\n",
    "            planned_lags : list of int\n",
    "                Lags of forecast, solar and wind data to use as predictors\n",
    "    \"\"\"\n",
    "    def __init__(self, model_params, lag_params):\n",
    "        self.lags = model_params['lags']\n",
    "        self.trend = model_params['trend']\n",
    "        self.ic = model_params['ic']\n",
    "        self.exog = model_params['exog']\n",
    "        \n",
    "        self.ida_price_lags = lag_params['ida_price_lags']\n",
    "        self.planned_lags = lag_params['planned_lags']\n",
    "        self.fuel_mix_lags = lag_params['fuel_mix_lags']\n",
    "        self.gas_price_lags = lag_params['gas_price_lags']\n",
    "\n",
    "    def ingest_data(self, train_target, train_ida, train_planned, train_fuel_mix, train_gas_price):\n",
    "        start_dates = [df.index[0] for df in [train_target, train_ida, train_planned, train_fuel_mix, train_gas_price]]\n",
    "        latest_start_date = max(start_dates)\n",
    "\n",
    "        # Split into training predictors and test predictors\n",
    "        last_day_of_data = dt.datetime.combine(train_planned.index.date[-1], dt.datetime.min.time())\n",
    "        test_index = pd.date_range(start=last_day_of_data, end=last_day_of_data+dt.timedelta(hours=23), freq='H')\n",
    "    \n",
    "        # Initialise predictors dataframe\n",
    "        predictors = pd.DataFrame(index=train_planned.index)\n",
    "        \n",
    "        # Build predictors from IDA Prices (1,2 & 3)\n",
    "        for column in train_ida:\n",
    "            for lag in self.ida_price_lags:\n",
    "                predictor_name = f\"{column}-{lag}\"\n",
    "                predictors.insert(predictors.shape[1], predictor_name, train_ida[column])\n",
    "                predictors[predictor_name] = predictors[predictor_name].shift(lag)\n",
    "\n",
    "        # Build predictors from Wind, Solar and Demand\n",
    "        for column in train_planned:\n",
    "            for lag in self.planned_lags:\n",
    "                predictor_name = f\"{column}-{lag}\"\n",
    "                predictors.insert(predictors.shape[1], predictor_name, train_planned[column])\n",
    "                predictors[predictor_name] = predictors[predictor_name].shift(lag)\n",
    "                \n",
    "         # Build predictors from Fuel Mix\n",
    "        for column in train_fuel_mix:\n",
    "            for lag in self.fuel_mix_lags:\n",
    "                predictor_name = f\"{column}-{lag}\"\n",
    "                predictors.insert(predictors.shape[1], predictor_name, train_fuel_mix[column])\n",
    "                predictors[predictor_name] = predictors[predictor_name].shift(lag)\n",
    "       \n",
    "        # Build predictors from Gas prices\n",
    "        for column in train_gas_price:\n",
    "            for lag in self.gas_price_lags:\n",
    "                predictor_name = f\"{column}-{lag}\"\n",
    "                predictors.insert(predictors.shape[1], predictor_name, train_gas_price[column])\n",
    "                predictors[predictor_name] = predictors[predictor_name].shift(lag)\n",
    "\n",
    "        # Split predictors into training and test predictors and store data for training and forecasting\n",
    "        train_predictors = predictors.drop(test_index)\n",
    "        test_predictors = predictors.loc[test_index,:]\n",
    "        \n",
    "        # Remove rows in training set with NAs\n",
    "        notna_train_predictors_loc = train_predictors.notna().all(axis=1)\n",
    "        train_predictors = train_predictors.loc[notna_train_predictors_loc]\n",
    "        train_target = train_target.loc[notna_train_predictors_loc]\n",
    "        \n",
    "        # Store data into object\n",
    "        self.train_predictors = train_predictors\n",
    "        self.test_predictors = test_predictors\n",
    "        self.train_target = train_target\n",
    "        \n",
    "        # Initialise dataframe for variable importances\n",
    "#         if not hasattr(self, \"variable_importances\"):\n",
    "#             self.variable_importances = pd.DataFrame(index=train_predictors.columns)\n",
    "        \n",
    "   \n",
    "    def train(self):\n",
    "        # Fit AR/ARX models with different lag values\n",
    "        if self.exog:\n",
    "            model_selection = jl.Parallel(n_jobs=-1, backend='threading') \\\n",
    "                (jl.delayed(tsa.ar_model.AutoReg(self.train_target.values,lag,self.trend,exog=self.train_predictors.values).fit)() for lag in self.lags)\n",
    "        else:\n",
    "            model_selection = jl.Parallel(n_jobs=-1, backend='threading') \\\n",
    "                (jl.delayed(tsa.ar_model.AutoReg(self.train_target.values,lag,self.trend).fit)() for lag in self.lags)\n",
    "        \n",
    "        # Pick best model based on the specified information criterion (AIC or BIC)\n",
    "        ar_ics = [model.aic for model in model_selection] if self.ic=='aic' else [model.bic for model in model_selection]\n",
    "        self.model = model_selection[ar_ics.index(min(ar_ics))]\n",
    "        \n",
    "    def forecast(self):\n",
    "        # Make forecasts\n",
    "        if self.exog:\n",
    "            forecast = self.model.predict(start=self.train_target.shape[0], end=self.train_target.shape[0]+23, exog_oos=self.test_predictors)\n",
    "        else:\n",
    "            forecast = self.model.predict(start=self.train_target.shape[0], end=self.train_target.shape[0]+23)\n",
    "        \n",
    "        # Store forecasts in labelled dataframe\n",
    "        forecast_df = pd.DataFrame(dict(Forecast=forecast), index=self.test_predictors.index)\n",
    "        return(forecast_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cab216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "def create_ffnn(num_of_nodes, input_cols, act_fn, n_layers=3, opt='adam', loss='mse'):\n",
    "    \"\"\"\n",
    "    Create a Keras neural network model (composed of Dense feedforward layers).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        num_of_nodes : int\n",
    "            Number of neurons per layer.\n",
    "        input_cols : int\n",
    "            Number of features/predictors.\n",
    "        act_fn : str in ['tanh', 'sigmoid', 'relu']\n",
    "            Activation function.\n",
    "        n_layers : int, default=3\n",
    "            Number of hidden layers.\n",
    "        opt : str, default='adam'\n",
    "            Optimiser for training.\n",
    "        loss : str, default='mse'\n",
    "            Loss function for training.\n",
    "    Returns\n",
    "    -------\n",
    "        model : tensorflow.python.keras.engine.sequential.Sequential\n",
    "            Sequential model representing the neural network model.\n",
    "    \"\"\"\n",
    "    # Initialise neural network model\n",
    "    model = Sequential()\n",
    "    initializer = initializers.he_uniform(1)\n",
    "    \n",
    "    # Add first hidden layer (with input layer specification)\n",
    "    model.add(Dense(num_of_nodes, activation=act_fn, input_shape=(input_cols,), kernel_initializer=initializer))\n",
    "    \n",
    "    # Add remaining hidden layers\n",
    "    for _ in range(n_layers-1):\n",
    "        model.add(Dense(num_of_nodes, activation=act_fn, kernel_initializer=initializer))\n",
    "\n",
    "    # Add output layer\n",
    "    model.add(Dense(1, kernel_initializer=initializer))\n",
    "\n",
    "    # Configure model optimizer and loss function\n",
    "    model.compile(optimizer=opt, loss=loss)\n",
    "\n",
    "    return(model)\n",
    "\n",
    "\n",
    "def scale_predictors(predictors, activation, copy_df=True):\n",
    "    \"\"\"\n",
    "    Rescale a dataframe of predictors according to the given activation function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        predictors : pandas.DataFrame\n",
    "        activation : str in ['tanh', 'sigmoid', 'relu']\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        scaler : sklearn.preprocessing._data.MinMaxScaler\n",
    "        scaled_predictors : pandas.DataFrame\n",
    "    \"\"\"\n",
    "    activation_ranges = {\n",
    "        'tanh': (-1,1),\n",
    "        'sigmoid': (0,1),\n",
    "        'relu': (0,5)\n",
    "    }\n",
    "    # Initialise scaler\n",
    "    scaler = prep.MinMaxScaler(feature_range=activation_ranges[activation], copy=copy_df)\n",
    "    \n",
    "    # Fit scaler and scale the data\n",
    "    scaled_predictors = pd.DataFrame(scaler.fit_transform(predictors), index=predictors.index, columns=predictors.columns)\n",
    "    \n",
    "    return(scaler, scaled_predictors)\n",
    "\n",
    "\n",
    "class ffnn():\n",
    "    \"\"\"\n",
    "    Feedforward neural network model implementation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        model_params : dict\n",
    "            init_params : dict\n",
    "                Arguments to pass into create_ffnn() function\n",
    "            train_params : dict\n",
    "                Extra arguments to pass into Sequential.fit() function\n",
    "            other_params : dict\n",
    "                Other extra arguments (number of epochs and bool to specify whether to use GPU)\n",
    "        lag_params : dict\n",
    "            price_lags : list of int\n",
    "                Lags of day-ahead market price data to use as predictors\n",
    "            ida_price_lags : list of int\n",
    "                Lags of balancing market price data to use as predictors\n",
    "            planned_lags : list of int\n",
    "                Lags of forecast and wind data to use as predictors\n",
    "    \"\"\"\n",
    "    def __init__(self, model_params, lag_params):\n",
    "        self.init_params = model_params['init_params']\n",
    "        self.train_params = model_params['train_params']\n",
    "        self.other_params = model_params['other_params']\n",
    "        \n",
    "        # Set to GPU/CPU\n",
    "        self.device = '/device:GPU:0' if self.other_params['GPU'] else '/CPU:0'\n",
    "        \n",
    "        if not hasattr(self, 'model'):\n",
    "            self.model = create_ffnn(**self.init_params)\n",
    "        \n",
    "        self.price_lags = lag_params['price_lags']\n",
    "        self.ida_price_lags = lag_params['ida_price_lags']\n",
    "        self.planned_lags = lag_params['planned_lags']\n",
    "        self.fuel_mix_lags = lag_params['fuel_mix_lags']\n",
    "        self.gas_price_lags = lag_params['gas_price_lags']\n",
    "        \n",
    "    def ingest_data(self, train_target, train_ida, train_planned, train_fuel_mix, train_gas_price):\n",
    "        start_dates = [df.index[0] for df in [train_target, train_ida, train_planned, train_fuel_mix, train_gas_price]]\n",
    "        latest_start_date = max(start_dates)\n",
    "\n",
    "        # Split into training predictors and test predictors\n",
    "        last_day_of_data = dt.datetime.combine(train_planned.index.date[-1], dt.datetime.min.time())\n",
    "        test_index = pd.date_range(start=last_day_of_data, end=last_day_of_data+dt.timedelta(hours=23), freq='H')\n",
    "\n",
    "        # Initialise predictors dataframe\n",
    "        predictors = pd.DataFrame(index=train_planned.index)\n",
    "\n",
    "        # Build predictors from EURPrices\n",
    "        for lag in self.price_lags:\n",
    "            predictor_name = f\"{train_target.columns[0]}-{lag}\"\n",
    "            predictors.insert(predictors.shape[1], predictor_name, train_target)\n",
    "            predictors[predictor_name] = predictors[predictor_name].shift(lag)\n",
    "        \n",
    "        #print(f\"Predictors after adding price lags: {predictors.shape}\")\n",
    "\n",
    "        # Build predictors from IDA Priceprint(f\"Predictors after adding IDA lags: {predictors.shape}\")\n",
    "        for column in train_ida:\n",
    "            for lag in self.ida_price_lags:\n",
    "                predictor_name = f\"{column}-{lag}\"\n",
    "                predictors.insert(predictors.shape[1], predictor_name, train_ida[column])\n",
    "                predictors[predictor_name] = predictors[predictor_name].shift(lag)\n",
    "        \n",
    "        #print(f\"Predictors after adding IDA lags: {predictors.shape}\")\n",
    "                \n",
    "        # Build predictors from Wind, solar and Demand\n",
    "        for column in train_planned:\n",
    "            for lag in self.planned_lags:\n",
    "                predictor_name = f\"{column}-{lag}\"\n",
    "                predictors.insert(predictors.shape[1], predictor_name, train_planned[column])\n",
    "                predictors[predictor_name] = predictors[predictor_name].shift(lag)\n",
    "        \n",
    "        #print(f\"Predictors after adding planned lags: {predictors.shape}\")\n",
    "        \n",
    "        # Build predictors from Fuel Mix\n",
    "        for column in train_fuel_mix:\n",
    "            for lag in self.fuel_mix_lags:\n",
    "                predictor_name = f\"{column}-{lag}\"\n",
    "                predictors.insert(predictors.shape[1], predictor_name, train_fuel_mix[column])\n",
    "                predictors[predictor_name] = predictors[predictor_name].shift(lag)\n",
    "       \n",
    "        # Build predictors from Gas prices\n",
    "        for column in train_gas_price:\n",
    "            for lag in self.gas_price_lags:\n",
    "                predictor_name = f\"{column}-{lag}\"\n",
    "                predictors.insert(predictors.shape[1], predictor_name, train_gas_price[column])\n",
    "                predictors[predictor_name] = predictors[predictor_name].shift(lag)\n",
    "                \n",
    "        # Split predictors into training and test predictors and store data for training and forecasting\n",
    "        train_predictors = predictors.drop(test_index)\n",
    "        test_predictors = predictors.loc[test_index,:]\n",
    "        \n",
    "        #print(f\"Training predictors shape: {train_predictors.shape}\")\n",
    "        #print(f\"Test predictors shape: {test_predictors.shape}\")\n",
    "\n",
    "        # Remove rows in training set with NAs\n",
    "        notna_train_predictors_loc = train_predictors.notna().all(axis=1)\n",
    "        train_predictors = train_predictors.loc[notna_train_predictors_loc]\n",
    "        train_target = train_target.loc[notna_train_predictors_loc]\n",
    "        \n",
    "        #print(f\"Training predictors shape after dropping NAs: {train_predictors.shape}\")\n",
    "\n",
    "        # Scale predictors and target, and store in object\n",
    "        self.train_predictors_scaler, self.scaled_train_predictors = scale_predictors(train_predictors, activation=self.init_params['act_fn'])\n",
    "        self.train_target_scaler, self.scaled_train_target = scale_predictors(train_target, activation=self.init_params['act_fn'])\n",
    "        self.scaled_test_predictors = self.train_predictors_scaler.transform(test_predictors)\n",
    "        \n",
    "        self.test_index = test_predictors.index\n",
    "        \n",
    "        # Initialise dataframe for variable importances\n",
    "        if not hasattr(self, 'variable_importances'):\n",
    "            self.variable_importances = pd.DataFrame(index=train_predictors.columns)\n",
    "            \n",
    "        # Calculate total number of features\n",
    "        total_features = (1 * len(self.price_lags)) + (train_ida.shape[1] * len(self.ida_price_lags)) + (train_planned.shape[1] * len(self.planned_lags)) + (train_fuel_mix.shape[1] * len(self.fuel_mix_lags)) + (train_gas_price.shape[1] * len(self.gas_price_lags))\n",
    "    \n",
    "        # Verify the input shape matches total features\n",
    "        assert self.scaled_train_predictors.shape[1] == total_features, f\"Expected input shape of {total_features}, but got {self.scaled_train_predictors.shape[1]}\"\n",
    "\n",
    "        # Initialise dataframe for variable importances\n",
    "        if not hasattr(self, 'variable_importances'):\n",
    "            self.variable_importances = pd.DataFrame(index=train_predictors.columns)\n",
    "\n",
    "        # Dynamically set the input_cols parameter in the model\n",
    "        self.model = create_ffnn(self.init_params['num_of_nodes'], total_features, self.init_params['act_fn'], self.init_params['n_layers'], self.init_params['opt'], self.init_params['loss'])\n",
    "\n",
    "    def train(self):\n",
    "        # Fit the forecasting model\n",
    "        if hasattr(self, 'history'):\n",
    "            self.history = self.model.fit(x=self.scaled_train_predictors.values, y=self.scaled_train_target.values, epochs=self.other_params['subseq_epochs'], batch_size=len(self.scaled_train_target), **self.train_params)\n",
    "        else:\n",
    "            self.history = self.model.fit(x=self.scaled_train_predictors.values, y=self.scaled_train_target.values, epochs=self.other_params['init_epochs'], batch_size=len(self.scaled_train_target), **self.train_params)\n",
    "        \n",
    "    def forecast(self):\n",
    "        forecast = self.train_target_scaler.inverse_transform(self.model.predict(x=self.scaled_test_predictors))\n",
    "        forecast_df = pd.DataFrame(forecast, index=self.test_index)\n",
    "        return(forecast_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1b151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "def create_data_window(data, n_steps, step_size=24):\n",
    "    \"\"\"\n",
    "    Given a (single column) predictor dataframe, returns a dataframe where each column is a lagged version\n",
    "    of the original column. This is part of the pre-processing step for reformatting data as RNN input/s.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        data : pandas.DataFrame\n",
    "        n_steps : int\n",
    "            Maximum number of steps to shift back. \n",
    "        step_size : int, default=24\n",
    "    \"\"\"\n",
    "    new_data = data.copy()\n",
    "    column = data.columns[0]\n",
    "\n",
    "    # Add lagged values as new columns\n",
    "    for step in range(n_steps):\n",
    "        new_data.insert(new_data.shape[1], f\"{column}-{step_size*(step+1)}h\", new_data[[column]].shift(step_size * (step+1)))\n",
    "    \n",
    "    return(new_data)\n",
    "\n",
    "\n",
    "def get_rnn_scaler(predictors, activation, copy_df=True):\n",
    "    \"\"\"\n",
    "    Given a dataframe, return a scaler that can be used on other dataframes. This function is used\n",
    "    for the predictors. The scaler is fit on (and used to transform) the train predictors and then\n",
    "    used to transform the test predictors.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        predictors : pandas.DataFrame\n",
    "        activation : str in ['tanh', 'sigmoid', 'relu']\n",
    "        copy_df : bool, default=True\n",
    "            Set to False to perform inplace row normalization and avoid a\n",
    "            copy (if the input is already a numpy array).\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "        scaler : sklearn.preprocessing._data.MinMaxScaler\n",
    "    \"\"\"\n",
    "    activation_ranges = {\n",
    "        'tanh': (-1,1),\n",
    "        'sigmoid': (0,1),\n",
    "        'relu': (0,5)\n",
    "    }\n",
    "    real_predictors = predictors[:,0,:].copy()\n",
    "    \n",
    "    # Create and fit scaler\n",
    "    scaler = prep.MinMaxScaler(feature_range=activation_ranges[activation], copy=copy_df)\n",
    "    scaler.fit(real_predictors)\n",
    "    \n",
    "    return(scaler)\n",
    "\n",
    "\n",
    "def transform_rnn_scale(predictors, scaler):\n",
    "    \"\"\"\n",
    "    Function to apply the scaler to the array of predictors. This function is needed instead\n",
    "    of the usual scaler.transform() method because predictors is a 3D numpy array instead\n",
    "    of the 2D dataframe/array expected by the function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        predictors : numpy.array\n",
    "        scaler : sklearn.preprocessing._data.MinMaxScaler\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        scaled_predictors : numpy.array\n",
    "    \"\"\"\n",
    "    # Transform predictors\n",
    "    scaled_predictors = predictors.copy()\n",
    "    \n",
    "    for time_step in range(scaled_predictors.shape[1]):\n",
    "        scaled_predictors[:,time_step,:] = scaler.transform(scaled_predictors[:,time_step,:])\n",
    "        \n",
    "    return(scaled_predictors)\n",
    "\n",
    "\n",
    "def create_rnn(num_of_blocks, n_timesteps, n_features, act_fn, n_layers=3, opt='adam', loss='mse'):\n",
    "    \"\"\"\n",
    "    Create a Keras neural network model (composed of LSTM layers).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        num_of_blocks : int\n",
    "            Number of neurons per layer.\n",
    "        input_cols : int\n",
    "            Number of features/predictors (excluding lagged predictors).\n",
    "        act_fn : str in ['tanh', 'sigmoid', 'relu']\n",
    "            Activation function.\n",
    "        n_layers : int, default=3\n",
    "            Number of hidden layers.\n",
    "        opt : str, default='adam'\n",
    "            Optimiser for training.\n",
    "        loss : str, default='mse'\n",
    "            Loss function for training.\n",
    "    Returns\n",
    "    -------\n",
    "        model : tensorflow.python.keras.engine.sequential.Sequential\n",
    "            Sequential model representing the neural network model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add first hidden layer\n",
    "    if n_layers == 1:\n",
    "        model.add(LSTM(num_of_blocks, activation=act_fn, input_shape=(n_timesteps, n_features)))\n",
    "    else:\n",
    "        model.add(LSTM(num_of_blocks, return_sequences=True, activation=act_fn, input_shape=(n_timesteps, n_features)))\n",
    "    \n",
    "    # Add remaining hidden layers\n",
    "    for n in range(n_layers-1):\n",
    "        if n == n_layers-2:\n",
    "            model.add(LSTM(num_of_blocks, activation=act_fn))\n",
    "        else:\n",
    "            model.add(LSTM(num_of_blocks, return_sequences=True, activation=act_fn))\n",
    "\n",
    "    # Add output layer\n",
    "    model.add(Dense(1))#, kernel_initializer=initializer))\n",
    "\n",
    "    # Configure model optimizer and loss function\n",
    "    model.compile(optimizer=opt, loss=loss)\n",
    "\n",
    "    return(model)\n",
    "\n",
    "\n",
    "class rnn():\n",
    "    \"\"\"\n",
    "    Recurrent neural network (LSTM) model implementation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        model_params : dict\n",
    "            init_params : dict\n",
    "                Arguments to pass into create_rnn() function\n",
    "            train_params : dict\n",
    "                Extra arguments to pass into Sequential.fit() function\n",
    "            other_params : dict\n",
    "                Other extra arguments (number of epochs and bool to specify whether to use GPU)\n",
    "    \"\"\"\n",
    "    def __init__(self, model_params, lag_params):\n",
    "        self.init_params = model_params['init_params']\n",
    "        self.train_params = model_params['train_params']\n",
    "        self.other_params = model_params['other_params']\n",
    "        \n",
    "        # Set to GPU/CPU\n",
    "        self.device = '/device:GPU:0' if self.other_params['GPU'] else '/CPU:0'\n",
    "        \n",
    "        if not hasattr(self, 'model'):\n",
    "            self.model = create_rnn(**self.init_params)\n",
    "        \n",
    "    def ingest_data(self, train_target, train_ida, train_planned, train_fuel_mix):\n",
    "        # Get the the latest start date of the variables\n",
    "        start_dates = [df.index[0] for df in [train_target, train_ida, train_planned]]\n",
    "        latest_start_date = max(start_dates)\n",
    "        target_end_date = train_target.index[-1]\n",
    "\n",
    "        # Split up planned data into its component columns\n",
    "        train_wind = train_planned.loc[:,'Wind'].to_frame()\n",
    "        train_demand = train_planned.loc[:,'Demand'].to_frame()\n",
    "        train_solar = train_planned.loc[:,'Solar'].to_frame()\n",
    "        \n",
    "        # Split up IDA data into its component columns\n",
    "        train_ida1 = train_ida.loc[:,'IDA1_Price'].to_frame()\n",
    "        train_ida2 = train_ida.loc[:,'IDA2_Price'].to_frame()\n",
    "        train_ida3 = train_ida.loc[:,'IDA3_Price'].to_frame()\n",
    "        \n",
    "        # Add extra rows corresponding to forecast dates\n",
    "        empty_frame = pd.DataFrame(index=train_planned.index[-24:])\n",
    "        train_target = pd.concat([train_target, empty_frame])\n",
    "        train_ida1 = pd.concat([train_ida1, empty_frame])\n",
    "        train_ida2 = pd.concat([train_ida2, empty_frame])\n",
    "        train_ida3 = pd.concat([train_ida3, empty_frame])\n",
    "        \n",
    "        # Reformat data for input into RNNs\n",
    "        n_timesteps = self.init_params['n_timesteps']\n",
    "        rnn_train_prices = create_data_window(train_target, n_timesteps)\n",
    "\n",
    "        rnn_train_target = rnn_train_prices[[rnn_train_prices.columns[0]]]\n",
    "        rnn_train_prices.drop(rnn_train_prices.columns[0], axis=1, inplace=True)\n",
    "\n",
    "        rnn_train_ida1 = create_data_window(train_ida1, n_timesteps)\n",
    "        rnn_train_ida1.drop(rnn_train_ida1.columns[0], axis=1, inplace=True)\n",
    "        \n",
    "        rnn_train_ida2 = create_data_window(train_ida2, n_timesteps)\n",
    "        rnn_train_ida2.drop(rnn_train_ida2.columns[0], axis=1, inplace=True)\n",
    "        \n",
    "        rnn_train_ida3 = create_data_window(train_ida3, n_timesteps)\n",
    "        rnn_train_ida3.drop(rnn_train_ida3.columns[0], axis=1, inplace=True)\n",
    "\n",
    "        rnn_train_wind = create_data_window(train_wind, n_timesteps)\n",
    "        rnn_train_wind.drop(rnn_train_wind.columns[-1], axis=1, inplace=True)\n",
    "\n",
    "        rnn_train_demand = create_data_window(train_demand, n_timesteps)\n",
    "        rnn_train_demand.drop(rnn_train_demand.columns[-1], axis=1, inplace=True)\n",
    "        \n",
    "        rnn_train_solar = create_data_window(train_solar, n_timesteps)\n",
    "        rnn_train_solar.drop(rnn_train_solar.columns[-1], axis=1, inplace=True)\n",
    "               \n",
    "        # Remove all rows with NAs that arose due to the reformatting above\n",
    "        rnn_train_prices = rnn_train_prices.loc[rnn_train_prices.notna().all(axis=1)]\n",
    "        rnn_train_target = rnn_train_target.loc[rnn_train_target.notna().all(axis=1)]\n",
    "        rnn_train_ida1 = rnn_train_ida1.loc[rnn_train_ida1.notna().all(axis=1)]\n",
    "        rnn_train_ida2 = rnn_train_ida2.loc[rnn_train_ida2.notna().all(axis=1)]\n",
    "        rnn_train_ida3 = rnn_train_ida3.loc[rnn_train_ida3.notna().all(axis=1)]\n",
    "        rnn_train_wind = rnn_train_wind.loc[rnn_train_wind.notna().all(axis=1)]\n",
    "        rnn_train_demand = rnn_train_demand.loc[rnn_train_demand.notna().all(axis=1)]\n",
    "        rnn_train_solar = rnn_train_solar.loc[rnn_train_solar.notna().all(axis=1)]\n",
    "        \n",
    "        # Match data start dates (again)\n",
    "        #min_start_index = max(rnn_train_prices.index[0], rnn_train_ida1.index[0], rnn_train_ida2.index[0], rnn_train_ida3.index[0], rnn_train_wind.index[0], rnn_train_demand.index[0], rnn_train_solar.index[0])\n",
    "        rnn_train_target = rnn_train_target.loc[rnn_train_target.index >= rnn_train_prices.index[0]]\n",
    "        rnn_train_ida1 = rnn_train_ida1.loc[rnn_train_ida1.index >= rnn_train_prices.index[0]]\n",
    "        rnn_train_ida2 = rnn_train_ida2.loc[rnn_train_ida2.index >= rnn_train_prices.index[0]]\n",
    "        rnn_train_ida3 = rnn_train_ida3.loc[rnn_train_ida3.index >= rnn_train_prices.index[0]]\n",
    "        rnn_train_wind = rnn_train_wind.loc[rnn_train_wind.index >= rnn_train_prices.index[0]]\n",
    "        rnn_train_demand = rnn_train_demand.loc[rnn_train_demand.index >= rnn_train_prices.index[0]]\n",
    "        rnn_train_solar = rnn_train_solar.loc[rnn_train_solar.index >= rnn_train_prices.index[0]]\n",
    "        \n",
    "        common_indices = rnn_train_prices.index.intersection(rnn_train_ida1.index).intersection(rnn_train_ida2.index).intersection(rnn_train_ida3.index).intersection(rnn_train_wind.index).intersection(rnn_train_demand.index).intersection(rnn_train_solar.index)\n",
    "        rnn_train_target = rnn_train_target.loc[rnn_train_target.index.isin(common_indices)]\n",
    "        rnn_train_prices = rnn_train_prices.loc[rnn_train_prices.index.isin(common_indices)]\n",
    "        rnn_train_ida1 = rnn_train_ida1.loc[rnn_train_ida1.index.isin(common_indices)]\n",
    "        rnn_train_ida2 = rnn_train_ida2.loc[rnn_train_ida2.index.isin(common_indices)]\n",
    "        rnn_train_ida3 = rnn_train_ida3.loc[rnn_train_ida3.index.isin(common_indices)]\n",
    "        rnn_train_wind = rnn_train_wind.loc[rnn_train_wind.index.isin(common_indices)]\n",
    "        rnn_train_demand = rnn_train_demand.loc[rnn_train_demand.index.isin(common_indices)]\n",
    "        rnn_train_solar = rnn_train_solar.loc[rnn_train_solar.index.isin(common_indices)]\n",
    "        \n",
    "        # Split into training and test set\n",
    "        self.test_index = rnn_train_prices.index[-24:]\n",
    "\n",
    "        # Test data\n",
    "        rnn_test_prices = rnn_train_prices.loc[rnn_train_prices.index >= self.test_index[0]]\n",
    "        rnn_test_ida1 = rnn_train_ida1.loc[rnn_train_ida1.index >= self.test_index[0]]\n",
    "        rnn_test_ida2 = rnn_train_ida2.loc[rnn_train_ida2.index >= self.test_index[0]]\n",
    "        rnn_test_ida3 = rnn_train_ida3.loc[rnn_train_ida3.index >= self.test_index[0]]\n",
    "        rnn_test_wind = rnn_train_wind.loc[rnn_train_wind.index >= self.test_index[0]]\n",
    "        rnn_test_demand = rnn_train_demand.loc[rnn_train_demand.index >= self.test_index[0]]\n",
    "        rnn_test_solar = rnn_train_solar.loc[rnn_train_solar.index >= self.test_index[0]]\n",
    "\n",
    "        # Train data\n",
    "        rnn_train_prices = rnn_train_prices.loc[rnn_train_prices.index < self.test_index[0]]\n",
    "        rnn_train_ida1 = rnn_train_ida1.loc[rnn_train_ida1.index < self.test_index[0]]\n",
    "        rnn_train_ida2 = rnn_train_ida2.loc[rnn_train_ida2.index < self.test_index[0]]\n",
    "        rnn_train_ida3 = rnn_train_ida3.loc[rnn_train_ida3.index < self.test_index[0]]\n",
    "        rnn_train_wind = rnn_train_wind.loc[rnn_train_wind.index < self.test_index[0]]\n",
    "        rnn_train_demand = rnn_train_demand.loc[rnn_train_demand.index < self.test_index[0]]\n",
    "        rnn_train_solar = rnn_train_solar.loc[rnn_train_solar.index < self.test_index[0]]\n",
    "        rnn_train_target = rnn_train_target.loc[rnn_train_target.index < self.test_index[0]]\n",
    "        \n",
    "        # Combine train and test features into one tensor each\n",
    "        rnn_test_predictors = np.hstack((rnn_test_prices, rnn_test_ida1, rnn_test_ida2, rnn_test_ida3, rnn_test_wind, rnn_test_demand, rnn_test_solar)).reshape(rnn_test_prices.shape[0], 7, n_timesteps).transpose(0,2,1)\n",
    "        rnn_train_predictors = np.hstack((rnn_train_prices, rnn_train_ida1, rnn_train_ida2, rnn_train_ida3, rnn_train_wind, rnn_train_demand, rnn_train_solar)).reshape(rnn_train_prices.shape[0], 7, n_timesteps).transpose(0,2,1)\n",
    "\n",
    "        # Scale predictors and target, and store in object\n",
    "        self.rnn_train_predictors_scaler = get_rnn_scaler(rnn_train_predictors, activation=self.init_params[\"act_fn\"])\n",
    "        self.rnn_scaled_train_predictors = transform_rnn_scale(rnn_train_predictors, self.rnn_train_predictors_scaler)\n",
    "        self.rnn_scaled_test_predictors = transform_rnn_scale(rnn_test_predictors, self.rnn_train_predictors_scaler)\n",
    "        self.rnn_train_target_scaler = get_rnn_scaler(rnn_train_target.values.reshape(-1, 1, 1), activation=self.init_params['act_fn'])\n",
    "        self.rnn_scaled_train_target = self.rnn_train_target_scaler.transform(rnn_train_target.values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        with tf.device(self.device):\n",
    "            epochs = self.other_params['subseq_epochs'] if hasattr(self, 'history') else self.other_params['init_epochs']\n",
    "            self.history = self.model.fit(x=self.rnn_scaled_train_predictors, y=self.rnn_scaled_train_target, epochs=epochs, **self.train_params)\n",
    "\n",
    "    def forecast(self):\n",
    "        forecast = self.rnn_train_target_scaler.inverse_transform(self.model.predict(x=self.rnn_scaled_test_predictors))\n",
    "        forecast_df = pd.DataFrame(forecast, index=self.test_index)\n",
    "        return forecast_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
